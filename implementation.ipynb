{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80: ε=0.14, Loss=64.9244, RMSE=72.4590\n",
      "Epoch 10/80: ε=1.36, Loss=13.6398, RMSE=31.4416\n",
      "Epoch 20/80: ε=2.71, Loss=2.4505, RMSE=8.2346\n",
      "Epoch 30/80: ε=4.07, Loss=1.4245, RMSE=1.9049\n",
      "Epoch 40/80: ε=5.43, Loss=1.0874, RMSE=1.3161\n",
      "Epoch 50/80: ε=6.79, Loss=0.8016, RMSE=1.2486\n",
      "Epoch 60/80: ε=8.14, Loss=0.7182, RMSE=1.2373\n",
      "Epoch 70/80: ε=9.50, Loss=0.6431, RMSE=1.2577\n",
      "Epoch 80/80: ε=10.86, Loss=0.6499, RMSE=1.2133\n",
      "Final Test RMSE: 1.2133, Total ε=10.86\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 1. Load and preprocess ML-100k with normalization\n",
    "def load_ml100k(base_path, split='u1'):\n",
    "    train = pd.read_csv(f'{base_path}/{split}.base', sep='\\t', header=None,\n",
    "                        names=['user','item','rating','ts'])\n",
    "    test = pd.read_csv(f'{base_path}/{split}.test', sep='\\t', header=None,\n",
    "                       names=['user','item','rating','ts'])\n",
    "    items = pd.read_csv(f'{base_path}/u.item', sep='|', header=None,\n",
    "                        encoding='latin-1', usecols=range(5,24))\n",
    "    P = torch.FloatTensor(items.values)\n",
    "\n",
    "    nb_users = int(max(train.user.max(), test.user.max()))\n",
    "    nb_items = int(max(train.item.max(), test.item.max()))\n",
    "\n",
    "    def to_matrix(df):\n",
    "        mat = np.zeros((nb_users, nb_items), dtype=np.float32)\n",
    "        mask = np.zeros_like(mat, dtype=np.float32)\n",
    "        for u,i,r,_ in df.itertuples(index=False):\n",
    "            mat[u-1,i-1] = r / 5.0  # Normalize to [0,1]\n",
    "            mask[u-1,i-1] = 1\n",
    "        return mat, mask\n",
    "\n",
    "    R_train, M_train = to_matrix(train)\n",
    "    R_test, M_test = to_matrix(test)\n",
    "    return (torch.FloatTensor(R_train), torch.FloatTensor(M_train),\n",
    "            torch.FloatTensor(R_test), torch.FloatTensor(M_test),\n",
    "            P, nb_users, nb_items)\n",
    "\n",
    "# Paths and device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_path = '/Users/sivamanipatnala/Desktop/Academics/Sem_6/RS/ml-100k'\n",
    "R_train, M_train, R_test, M_test, P, U, I = load_ml100k(base_path, split='u1')\n",
    "R_train, M_train, R_test, M_test, P = [x.to(device) for x in (R_train, M_train, R_test, M_test, P)]\n",
    "\n",
    "# 2. Model: Joint Dual Semi-AE\n",
    "class JointDPDAE(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_features, latent_dim):\n",
    "        super().__init__()\n",
    "        self.u_enc = nn.Sequential(\n",
    "            nn.Linear(n_items, 200), nn.Sigmoid(),\n",
    "            nn.Linear(200, latent_dim), nn.Sigmoid()\n",
    "        )\n",
    "        self.u_dec = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 200), nn.Sigmoid(),\n",
    "            nn.Linear(200, n_items)\n",
    "        )\n",
    "        self.i_enc = nn.Sequential(\n",
    "            nn.Linear(n_users + n_features, 200), nn.Sigmoid(),\n",
    "            nn.Linear(200, latent_dim), nn.Sigmoid()\n",
    "        )\n",
    "        self.i_dec = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 200), nn.Sigmoid(),\n",
    "            nn.Linear(200, n_users)\n",
    "        )\n",
    "\n",
    "    def forward(self, U_ratings, I_ratings, I_features):\n",
    "        U_lat = self.u_enc(U_ratings)\n",
    "        U_rec = self.u_dec(U_lat)\n",
    "        x = torch.cat([I_ratings, I_features], dim=1)\n",
    "        I_lat = self.i_enc(x)\n",
    "        I_rec = self.i_dec(I_lat)\n",
    "        return U_rec, U_lat, I_rec, I_lat\n",
    "\n",
    "# 3. Hyperparameters\n",
    "grad_batch_size = 128\n",
    "latent_dim = 64\n",
    "lambda_mf = 0.3\n",
    "reg_lambda = 1e-5\n",
    "lr = 0.005\n",
    "epochs = 80  \n",
    "max_grad_norm = 0.3\n",
    "epsilon = 4\n",
    "sample_rate = grad_batch_size / U  # 128 / 943 ≈ 0.136\n",
    "\n",
    "# Noise scale for Laplace DP-SGD\n",
    "noise_scale = max_grad_norm / epsilon  # 1.0 / 4.0 = 0.25\n",
    "\n",
    "# 4. Initialize model and optimizer\n",
    "model = JointDPDAE(U, I, P.size(1), latent_dim).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(R_train, M_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=grad_batch_size, shuffle=True)\n",
    "\n",
    "# 5. Training loop with DP-SGD (Laplace noise)\n",
    "total_epsilon = 0.0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    for batch_idx, (batch_R, batch_M) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Process batch-relevant items\n",
    "        batch_items = torch.unique(torch.where(batch_M)[1])\n",
    "        I_ratings = R_train.t()[batch_items]\n",
    "        I_features = P[batch_items]\n",
    "        mask_i = M_train.t()[batch_items]\n",
    "        \n",
    "        # Forward pass\n",
    "        U_rec, U_lat, I_rec, I_lat = model(batch_R, I_ratings, I_features)\n",
    "        \n",
    "        # Loss calculation\n",
    "        mask_u = batch_M\n",
    "        loss_u = ((U_rec - batch_R) ** 2 * mask_u).sum() / mask_u.sum()\n",
    "        loss_i = ((I_rec - I_ratings) ** 2 * mask_i).sum() / mask_i.sum()\n",
    "        I_lat_full = model.i_enc(torch.cat([R_train.t(), P], dim=1))\n",
    "        R_pred = torch.matmul(U_lat, I_lat_full.t())\n",
    "        loss_mf = ((R_pred - batch_R) ** 2 * mask_u).sum() / mask_u.sum()\n",
    "        l2_reg = sum(p.pow(2).sum() for p in model.parameters())\n",
    "        loss = loss_u + loss_i + lambda_mf * loss_mf + reg_lambda * l2_reg\n",
    "        #loss = 0.1*loss_u + 0.1*loss_i + 0.8*loss_mf + reg_lambda*l2_reg\n",
    "\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # DP-SGD: Clip gradients and add Laplace noise\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                noise = torch.distributions.Laplace(0, noise_scale).sample(p.grad.shape).to(device)\n",
    "                p.grad += noise\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    # Update privacy budget (approximate amplification by sampling)\n",
    "    per_step_epsilon = min(sample_rate, 2 * sample_rate / noise_scale)  # Simplified bound\n",
    "    total_epsilon += per_step_epsilon\n",
    "    \n",
    "    # Evaluation\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            U_rec, U_lat, I_rec, I_lat = model(R_train, R_train.t(), P)\n",
    "            R_pred = torch.matmul(U_lat, I_lat.t()) * 5.0\n",
    "            se = ((R_pred - (R_test * 5.0)) ** 2 * M_test).sum()\n",
    "            rmse = torch.sqrt(se / M_test.sum()).item()\n",
    "        print(f\"Epoch {epoch}/{epochs}: ε={total_epsilon:.2f}, Loss={loss.item():.4f}, RMSE={rmse:.4f}\")\n",
    "\n",
    "# 6. Final Test RMSE\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    U_rec, U_lat, I_rec, I_lat = model(R_train, R_train.t(), P)\n",
    "    R_pred = torch.matmul(U_lat, I_lat.t()) * 5.0\n",
    "    se = ((R_pred - (R_test * 5.0)) ** 2 * M_test).sum()\n",
    "    rmse = torch.sqrt(se / M_test.sum()).item()\n",
    "print(f\"Final Test RMSE: {rmse:.4f}, Total ε={total_epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPOROVED WITH LESS NOISE FOR MORE POPULAR ITEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80: ε=0.03, RMSE=78.7577, Best=78.7577\n",
      "Epoch 10/80: ε=0.34, RMSE=44.9710, Best=44.9710\n",
      "Epoch 20/80: ε=0.68, RMSE=16.1233, Best=16.1233\n",
      "Epoch 30/80: ε=1.02, RMSE=5.4124, Best=5.4124\n",
      "Epoch 40/80: ε=1.36, RMSE=1.6129, Best=1.6129\n",
      "Epoch 50/80: ε=1.70, RMSE=1.7161, Best=1.6129\n",
      "Epoch 60/80: ε=2.04, RMSE=1.4999, Best=1.4999\n",
      "Epoch 70/80: ε=2.38, RMSE=1.4861, Best=1.4861\n",
      "Epoch 80/80: ε=2.71, RMSE=1.3449, Best=1.3449\n",
      "\n",
      "Final RMSE: 1.3449 (Best: 1.3449) at ε=2.71\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 1. Data Loading (Unchanged)\n",
    "def load_ml100k(base_path, split='u1'):\n",
    "    train = pd.read_csv(f'{base_path}/{split}.base', sep='\\t', header=None,\n",
    "                       names=['user','item','rating','ts'])\n",
    "    test = pd.read_csv(f'{base_path}/{split}.test', sep='\\t', header=None,\n",
    "                      names=['user','item','rating','ts'])\n",
    "    items = pd.read_csv(f'{base_path}/u.item', sep='|', header=None,\n",
    "                       encoding='latin-1', usecols=range(5,24))\n",
    "    P = torch.FloatTensor(items.values)\n",
    "\n",
    "    nb_users = int(max(train.user.max(), test.user.max()))\n",
    "    nb_items = int(max(train.item.max(), test.item.max()))\n",
    "\n",
    "    def to_matrix(df):\n",
    "        mat = np.zeros((nb_users, nb_items), dtype=np.float32)\n",
    "        mask = np.zeros_like(mat, dtype=np.float32)\n",
    "        for u,i,r,_ in df.itertuples(index=False):\n",
    "            mat[u-1,i-1] = r / 5.0\n",
    "            mask[u-1,i-1] = 1\n",
    "        return mat, mask\n",
    "\n",
    "    R_train, M_train = to_matrix(train)\n",
    "    R_test, M_test = to_matrix(test)\n",
    "    return (torch.FloatTensor(R_train), torch.FloatTensor(M_train),\n",
    "            torch.FloatTensor(R_test), torch.FloatTensor(M_test),\n",
    "            P, nb_users, nb_items)\n",
    "\n",
    "# 2. Hyperparameters (Optimized)\n",
    "grad_batch_size = 128\n",
    "latent_dim = 64\n",
    "lambda_mf = 0.3          \n",
    "reg_lambda = 1e-5\n",
    "lr = 0.005\n",
    "epochs = 80\n",
    "max_grad_norm = 0.3      \n",
    "epsilon = 4\n",
    "denoise_prob = 0.1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_path = 'ml-100k'\n",
    "R_train, M_train, R_test, M_test, P, U, I = load_ml100k(base_path, split='u1')\n",
    "R_train, M_train, R_test, M_test, P = [x.to(device) for x in (R_train, M_train, R_test, M_test, P)]\n",
    "\n",
    "# 3. Model (No Forward Noise)\n",
    "class JointDPDAE_Corrected(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_features, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.u_dropout = nn.Dropout(denoise_prob)\n",
    "        self.i_dropout = nn.Dropout(denoise_prob)\n",
    "        \n",
    "        # User network\n",
    "        self.u_enc = nn.Sequential(\n",
    "            nn.Linear(n_items, 200), nn.Sigmoid(),\n",
    "            nn.Linear(200, latent_dim), nn.Sigmoid()\n",
    "        )\n",
    "        self.u_dec = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 200), nn.Sigmoid(),\n",
    "            nn.Linear(200, n_items)\n",
    "        )\n",
    "        \n",
    "        # Item network\n",
    "        self.i_enc = nn.Sequential(\n",
    "            nn.Linear(n_users + n_features, 200), nn.Sigmoid(),\n",
    "            nn.Linear(200, latent_dim), nn.Sigmoid()\n",
    "        )\n",
    "        self.i_dec = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 200), nn.Sigmoid(),\n",
    "            nn.Linear(200, n_users)\n",
    "        )\n",
    "\n",
    "    def forward(self, U_ratings, I_ratings, I_features):\n",
    "        U_noisy = self.u_dropout(U_ratings)\n",
    "        x = torch.cat([I_ratings, I_features], dim=1)\n",
    "        x_noisy = self.i_dropout(x)\n",
    "        \n",
    "        U_lat = self.u_enc(U_noisy)\n",
    "        I_lat = self.i_enc(x_noisy)\n",
    "        \n",
    "        U_rec = self.u_dec(U_lat)\n",
    "        I_rec = self.i_dec(I_lat)\n",
    "        \n",
    "        return U_rec, U_lat, I_rec, I_lat\n",
    "\n",
    "# 4. Initialize\n",
    "model = JointDPDAE_Corrected(U, I, P.size(1), latent_dim).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "train_loader = DataLoader(TensorDataset(R_train, M_train), \n",
    "                         batch_size=grad_batch_size, shuffle=True)\n",
    "\n",
    "# 5. Popularity-aware Noise Scaling\n",
    "pop_counts = M_train.sum(dim=0)\n",
    "pop_norm = (pop_counts / pop_counts.max()).clamp(min=0.1)\n",
    "noise_scales = (max_grad_norm / (epsilon * pop_norm))\n",
    "\n",
    "# 6. Training Loop (Corrected DP-SGD)\n",
    "total_epsilon = 0.0\n",
    "best_rmse = float('inf')\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    I_lat_full = model.i_enc(torch.cat([R_train.t(), P], dim=1)).detach()\n",
    "    \n",
    "    for batch_idx, (batch_R, batch_M) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get batch-specific items and their noise scales\n",
    "        batch_items = torch.unique(torch.where(batch_M)[1])\n",
    "        I_ratings = R_train.t()[batch_items]\n",
    "        I_features = P[batch_items]\n",
    "        batch_noise_scales = noise_scales[batch_items]\n",
    "\n",
    "        # Forward pass\n",
    "        U_rec, U_lat, I_rec, I_lat = model(batch_R, I_ratings, I_features)\n",
    "        \n",
    "        # Balanced loss calculation\n",
    "        mask_u = batch_M\n",
    "        loss_u = ((U_rec - batch_R)**2 * mask_u).sum() / mask_u.sum()\n",
    "        mask_i = M_train.t()[batch_items]\n",
    "        loss_i = ((I_rec - I_ratings)**2 * mask_i).sum() / mask_i.sum()\n",
    "        R_pred = U_lat @ I_lat_full.t()\n",
    "        loss_mf = ((R_pred - batch_R)**2 * mask_u).sum() / mask_u.sum()\n",
    "        l2_reg = sum(p.pow(2).sum() for p in model.parameters())\n",
    "        \n",
    "        loss = 0.4*loss_u + 0.4*loss_i + 0.2*loss_mf + reg_lambda*l2_reg\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Popularity-aware gradient processing\n",
    "        with torch.no_grad():\n",
    "            # Global gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            # Item encoder specific noise\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'i_enc' in name and param.grad is not None:\n",
    "                    # Average noise scale for batch items\n",
    "                    avg_scale = batch_noise_scales.mean()\n",
    "                    noise = torch.distributions.Laplace(0, avg_scale).sample(param.grad.shape)\n",
    "                    param.grad += noise.to(device)\n",
    "                \n",
    "                # Standard noise for other parameters\n",
    "                elif param.grad is not None:\n",
    "                    noise = torch.distributions.Laplace(0, max_grad_norm/epsilon).sample(param.grad.shape)\n",
    "                    param.grad += noise.to(device)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # Privacy accounting (simplified)\n",
    "    total_epsilon += (grad_batch_size/U) * (1/epsilon)\n",
    "    \n",
    "    # Validation\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            _, U_lat, _, I_lat = model(R_train, R_train.t(), P)\n",
    "            R_pred = (U_lat @ I_lat.t()) * 5.0\n",
    "            rmse = torch.sqrt(((R_pred - R_test*5.0)**2 * M_test).sum() / M_test.sum()).item()\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "            print(f\"Epoch {epoch}/{epochs}: ε={total_epsilon:.2f}, RMSE={rmse:.4f}, Best={best_rmse:.4f}\")\n",
    "\n",
    "# Final Test\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, U_lat, _, I_lat = model(R_train, R_train.t(), P)\n",
    "    R_pred = (U_lat @ I_lat.t()) * 5.0\n",
    "    rmse = torch.sqrt(((R_pred - R_test*5.0)**2 * M_test).sum() / M_test.sum()).item()\n",
    "print(f\"\\nFinal RMSE: {rmse:.4f} (Best: {best_rmse:.4f}) at ε={total_epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
